{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exercise - Putting it All Together\n",
    "\n",
    "In this last exercise, you'll write a full ETL pipeline for the GDP data. That means you'll extract the World Bank data, transform the data, and load the data all in one go. In other words, you'll want one Python script that can do the entire process.\n",
    "\n",
    "Why would you want to do this? Imagine working for a company that creates new data every day. As new data comes in, you'll want to write software that periodically and automatically extracts, transforms, and loads the data.\n",
    "\n",
    "To give you a sense for what this is like, you'll extract the GDP data one line at a time. You'll then transform that line of data and load the results into a SQLite database. The code in this exercise is somewhat tricky.\n",
    "\n",
    "Here is an explanation of how this Jupyter notebook is organized:\n",
    "1. The first cell connects to a SQLite database called worldbank.db and creates a table to hold the gdp data. You do not need to do anything in this code cell other than executing the cell.\n",
    "2. The second cell has a function called extract_line(). You don't need to do anything in this code cell either besides executing the cell. This function is a [Python generator](https://wiki.python.org/moin/Generators). You don't need to understand how this works in order to complete the exercise. Essentially, a generator is like a regular function except instead of a return statement, a generator has a yield statement. Generators allow you to use functions in a for loop. In essence, this function will allow you to read in a data file one line at a time, run a transformation on that row of data, and then move on to the next row in the file.\n",
    "3. The third cell contains a function called transform_indicator_data(). This function receives a line from the csv file and transforms the data in preparation for a load step.\n",
    "4. The fourth cell contains a function called load_indicator_data(), which loads the trasnformed data into the gdp table in the worldbank.db database.\n",
    "5. The fifth cell runs the ETL pipeilne\n",
    "6. The sixth cell runs a query against the database to make sure everything worked correctly.\n",
    "\n",
    "You'll need to modify the third and fourth cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to create a database and a table, called gdp, to hold the gdp data\n",
    "# You do not need to change anything in this code cell\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "# connect to the database\n",
    "# the database file will be worldbank.db\n",
    "# note that sqlite3 will create this database file if it does not exist already\n",
    "conn = sqlite3.connect('worldbank.db')\n",
    "\n",
    "# get a cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "# drop the test table in case it already exists\n",
    "cur.execute(\"DROP TABLE IF EXISTS gdp\")\n",
    "\n",
    "# create the test table including project_id as a primary key\n",
    "cur.execute(\"CREATE TABLE gdp (countryname TEXT, countrycode TEXT, year INTEGER, gdp REAL, PRIMARY KEY (countrycode, year));\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator for reading in one line at a time\n",
    "# generators are useful for data sets that are too large to fit in RAM\n",
    "# You do not need to change anything in this code cell\n",
    "def extract_lines(file):\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill out the code wherever you find a TODO in this cell\n",
    "# This function has two inputs:\n",
    "#   data, which is a row of data from the gdp csv file\n",
    "#   colnames, which is a list of column names from the csv file\n",
    "# The output should be a list of [countryname, countrycode, year, gdp] values\n",
    "# In other words, the output would look like:\n",
    "# [[Aruba, ABW, 1994, 1.330168e+09], [Aruba, ABW, 1995, 1.320670e+09], ...]\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "# transform the indicator data\n",
    "def transform_indicator_data(data, colnames):\n",
    "    \n",
    "    # get rid of quote marks\n",
    "    for i, datum in enumerate(data):\n",
    "        data[i] = datum.replace('\"','')\n",
    "    \n",
    "    # TODO: the data variable contains a list of data in the form [countryname, countrycode, 1960, 1961, 1962,...]\n",
    "    # since this is the format of the data in the csv file. Extract the countryname from the list \n",
    "    # and put the result in the country variable\n",
    "    country = data[0]\n",
    "    \n",
    "    # these are \"countryname\" values that are not actually countries\n",
    "    non_countries = ['World',\n",
    "     'High income',\n",
    "     'OECD members',\n",
    "     'Post-demographic dividend',\n",
    "     'IDA & IBRD total',\n",
    "     'Low & middle income',\n",
    "     'Middle income',\n",
    "     'IBRD only',\n",
    "     'East Asia & Pacific',\n",
    "     'Europe & Central Asia',\n",
    "     'North America',\n",
    "     'Upper middle income',\n",
    "     'Late-demographic dividend',\n",
    "     'European Union',\n",
    "     'East Asia & Pacific (excluding high income)',\n",
    "     'East Asia & Pacific (IDA & IBRD countries)',\n",
    "     'Euro area',\n",
    "     'Early-demographic dividend',\n",
    "     'Lower middle income',\n",
    "     'Latin America & Caribbean',\n",
    "     'Latin America & the Caribbean (IDA & IBRD countries)',\n",
    "     'Latin America & Caribbean (excluding high income)',\n",
    "     'Europe & Central Asia (IDA & IBRD countries)',\n",
    "     'Middle East & North Africa',\n",
    "     'Europe & Central Asia (excluding high income)',\n",
    "     'South Asia (IDA & IBRD)',\n",
    "     'South Asia',\n",
    "     'Arab World',\n",
    "     'IDA total',\n",
    "     'Sub-Saharan Africa',\n",
    "     'Sub-Saharan Africa (IDA & IBRD countries)',\n",
    "     'Sub-Saharan Africa (excluding high income)',\n",
    "     'Middle East & North Africa (excluding high income)',\n",
    "     'Middle East & North Africa (IDA & IBRD countries)',\n",
    "     'Central Europe and the Baltics',\n",
    "     'Pre-demographic dividend',\n",
    "     'IDA only',\n",
    "     'Least developed countries: UN classification',\n",
    "     'IDA blend',\n",
    "     'Fragile and conflict affected situations',\n",
    "     'Heavily indebted poor countries (HIPC)',\n",
    "     'Low income',\n",
    "     'Small states',\n",
    "     'Other small states',\n",
    "     'Not classified',\n",
    "     'Caribbean small states',\n",
    "     'Pacific island small states']\n",
    "    \n",
    "    # filter out country name values that are in the above list\n",
    "    if country not in non_countries:\n",
    "        \n",
    "        # In this section, you'll convert the single row of data into a data frame\n",
    "        # The advantage of converting a single row of data into a data frame is that you can\n",
    "        # re-use code from earlier in the lesson to clean the data\n",
    "        \n",
    "        # TODO: convert the data variable into a numpy array\n",
    "        # Use the ndmin=2 option\n",
    "        data_array = np.array(data, ndmin=2)\n",
    "        \n",
    "        # TODO: reshape the data_array so that it is one row and 63 columns\n",
    "        data_array = data_array.reshape(1, 63)\n",
    "        \n",
    "        # TODO: convert the data_array variable into a pandas dataframe\n",
    "        # Note that you can specify the column names as well using the colnames variable\n",
    "        # Also, replace all empty strings in the dataframe with nan (HINT: Use the replace module and np.nan)\n",
    "        df = pd.DataFrame(data_array, columns=colnames).replace('', np.nan)\n",
    "        \n",
    "        # TODO: Drop the 'Indicator Name' and 'Indicator Code' columns\n",
    "        df = df.drop(['\\n', 'Indicator Name', 'Indicator Code'], inplace=True, axis=1)\n",
    "        \n",
    "        # TODO: Reshape the data sets so that they are in long format\n",
    "        # The id_vars should be Country Name and Country Code\n",
    "        # You can name the variable column year and the value column gdp\n",
    "        # HINT: Use the pandas melt() method\n",
    "        # HINT: This was already done in a previous exercise\n",
    "        df_melt = pd.melt(df, id_vars=['Country Name', 'Country Code'],\n",
    "                            var_name='year',\n",
    "                            value_name='gdp')\n",
    "        \n",
    "        # TODO: Iterate through the rows in df_melt\n",
    "        # For each row, extract the country, countrycode, year, and gdp values into a list like this:\n",
    "        #     [country, countrycode, year, gdp]\n",
    "        # If the gdp value is not null, append the row (in the form of a list) to the results variable\n",
    "        # Finally, return the results list after iterating through the df_melt data\n",
    "        # HINT: the iterrows() method would be useful\n",
    "        # HINT: to check if gdp is equal to nan, you might want to convert gdp to a string and compare to the\n",
    "        #       string 'nan\n",
    "        results = []\n",
    "        for index, row in df_melt.iterrows():\n",
    "            country, countrycode, year, gdp = row\n",
    "            if str(gdp) != 'nan':\n",
    "                results.append([country, countrycode, year, gdp])\n",
    "\n",
    "        return results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill out the code wherever you find a TODO in this cell\n",
    "# This function loads data into the gdp table of the worldbank.db database\n",
    "# The input is a list of data outputted from the transformation step that looks like this:\n",
    "# [[Aruba, ABW, 1994, 1.330168e+09], [Aruba, ABW, 1995, 1.320670e+09], ...]\n",
    "\n",
    "# The function does not return anything. Instead, the function iterates through the input and inserts each\n",
    "# value into the gdp data set.\n",
    "\n",
    "def load_indicator_data(results):\n",
    "    \n",
    "    # TODO: connect to the worldbank.db database using the sqlite3 library\n",
    "    conn = sqlite3.connect('worldbank.db')\n",
    "    \n",
    "    # TODO: create a cursor object\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    if results:\n",
    "        \n",
    "        # iterate through the results variable and insert each result into the gdp table\n",
    "        for result in results:\n",
    "            \n",
    "            # TODO: extract the countryname, countrycode, year, and gdp from each iteration\n",
    "            countryname, countrycode, year, gdp = result\n",
    "\n",
    "            # TODO: prepare a query to insert a countryname, countrycode, year, gdp value\n",
    "            sql_string = 'INSERT INTO gdp (countryname, countrycode, year, gdp) VALUE (\"{}\", \"{}\", {}, {});'.format(countryname, countrycode, year, gdp)\n",
    "\n",
    "            # connect to database and execute query\n",
    "            try:\n",
    "                cur.execute(sql_string)\n",
    "            # print out any errors (like if the primary key constraint is violated)\n",
    "            except Exception as e:\n",
    "                print('error occurred:', e, result)\n",
    "    \n",
    "    # commit changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-5952e56e41fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;31m# transform and load the line of indicator data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_indicator_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0mload_indicator_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-d70e0d186239>\u001b[0m in \u001b[0;36mtransform_indicator_data\u001b[0;34m(data, colnames)\u001b[0m\n\u001b[1;32m    102\u001b[0m         df_melt = pd.melt(df, id_vars=['Country Name', 'Country Code'],\n\u001b[1;32m    103\u001b[0m                             \u001b[0mvar_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                             value_name='gdp')\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# TODO: Iterate through the rows in df_melt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/reshape/melt.py\u001b[0m in \u001b[0;36mmelt\u001b[0;34m(frame, id_vars, value_vars, var_name, value_name, col_level)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mid_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mid_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         elif (isinstance(frame.columns, ABCMultiIndex) and\n\u001b[0m\u001b[1;32m     32\u001b[0m               not isinstance(id_vars, list)):\n\u001b[1;32m     33\u001b[0m             raise ValueError('id_vars must be a list of tuples when columns'\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# Execute this code cell to run the ETL pipeline\n",
    "# You do not need to change anything in this cell\n",
    "\n",
    "# open the data file\n",
    "with open('../data/gdp_data.csv') as f:\n",
    "    # execute the generator to read in the file line by line\n",
    "    for line in extract_lines(f):\n",
    "        # split the comma separated values\n",
    "        data = line.split(',')\n",
    "        # check the length of the line because the first four lines of the csv file are not data\n",
    "        if len(data) == 63:\n",
    "            # check if the line represents column names\n",
    "            if data[0] == '\"Country Name\"':\n",
    "                colnames = []\n",
    "                # get rid of quote marks in the results to make the data easier to work with\n",
    "                for i, datum in enumerate(data):\n",
    "                    colnames.append(datum.replace('\"',''))\n",
    "            else:\n",
    "                # transform and load the line of indicator data\n",
    "                results = transform_indicator_data(data, colnames)\n",
    "                load_indicator_data(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>countryname</th>\n",
       "      <th>countrycode</th>\n",
       "      <th>year</th>\n",
       "      <th>gdp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [countryname, countrycode, year, gdp]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute this code cell to output the values in the gdp table\n",
    "# You do not need to change anything in this cell\n",
    "\n",
    "# connect to the database\n",
    "# the database file will be worldbank.db\n",
    "# note that sqlite3 will create this database file if it does not exist already\n",
    "conn = sqlite3.connect('worldbank.db')\n",
    "\n",
    "# get a cursor\n",
    "cur = conn.cursor()\n",
    "\n",
    "# create the test table including project_id as a primary key\n",
    "df = pd.read_sql(\"SELECT * FROM gdp\", con=conn)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "ETL Pipelines involve extracting data from one source, which in this case was a csv file, then transforming the data into a more usable form, and finally loading the data somewhere else.\n",
    "\n",
    "The purpose of ETL pipelines is to make data more usable and accessible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
